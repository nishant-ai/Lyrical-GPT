{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXMFmdeQFug4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "_qn6rMPwF9sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "MODEL_NAME = \"gpt2\"  # Options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
        "MAX_LENGTH = 1024    # Maximum context window size for GPT-2\n",
        "BATCH_SIZE = 4       # Batch size for training\n",
        "EPOCHS = 3           # Number of training epochs\n",
        "LEARNING_RATE = 3e-5 # Learning rate\n",
        "WARMUP_STEPS = 500   # Number of warmup steps for learning rate scheduler\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # Number of update steps to accumulate before performing a backward/update pass\n"
      ],
      "metadata": {
        "id": "7LVQHAazGGul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rZSM6SiWJleE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Custom dataset class for pre-tokenized data without padding\n",
        "class RawTokenDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.examples = []\n",
        "\n",
        "        logger.info(f\"Loading pre-tokenized data from {file_path}\")\n",
        "\n",
        "        # Load all token sequences from file\n",
        "        # Assuming each line contains a sequence of space-separated token IDs\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                # Convert token IDs to integers\n",
        "                token_ids = [int(token) for token in line.strip().split()]\n",
        "                self.examples.append(token_ids)\n",
        "\n",
        "        logger.info(f\"Loaded {len(self.examples)} sequences\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Simply return the token IDs as input_ids\n",
        "        return {\"input_ids\": torch.tensor(self.examples[idx])}"
      ],
      "metadata": {
        "id": "09ZRyxOrGWGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collator that handles variable-length sequences without padding\n",
        "class DynamicCollator:\n",
        "    def __init__(self, model_name):\n",
        "        # Load tokenizer just to get special token IDs\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        if not hasattr(self.tokenizer, 'pad_token') or self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        # Find the max length in this batch\n",
        "        batch_max_length = max(len(example[\"input_ids\"]) for example in examples)\n",
        "\n",
        "        # Prepare batch tensors\n",
        "        input_ids_batch = []\n",
        "        attention_mask_batch = []\n",
        "        labels_batch = []\n",
        "\n",
        "        for example in examples:\n",
        "            input_ids = example[\"input_ids\"]\n",
        "            input_len = len(input_ids)\n",
        "\n",
        "            # Create attention mask (1 for real tokens, 0 for padding)\n",
        "            attention_mask = torch.ones(batch_max_length, dtype=torch.long)\n",
        "\n",
        "            # Pad input_ids if needed\n",
        "            if input_len < batch_max_length:\n",
        "                # Create padded input_ids tensor\n",
        "                padded_input_ids = torch.cat([\n",
        "                    input_ids,\n",
        "                    torch.full((batch_max_length - input_len,), self.pad_token_id, dtype=torch.long)\n",
        "                ])\n",
        "\n",
        "                # Update attention mask for padding\n",
        "                attention_mask[input_len:] = 0\n",
        "            else:\n",
        "                padded_input_ids = input_ids\n",
        "\n",
        "            # For labels, we'll use -100 for padding tokens (HuggingFace ignores these in loss calculation)\n",
        "            labels = padded_input_ids.clone()\n",
        "            if input_len < batch_max_length:\n",
        "                labels[input_len:] = -100\n",
        "\n",
        "            input_ids_batch.append(padded_input_ids)\n",
        "            attention_mask_batch.append(attention_mask)\n",
        "            labels_batch.append(labels)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids_batch),\n",
        "            \"attention_mask\": torch.stack(attention_mask_batch),\n",
        "            \"labels\": torch.stack(labels_batch)\n",
        "        }\n",
        "\n",
        "# Load tokenizer and model\n",
        "def get_tokenizer():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "    # We still define a pad token for the tokenizer API, but won't use it\n",
        "    if not hasattr(tokenizer, 'pad_token') or tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    return tokenizer\n",
        "\n",
        "def get_model():\n",
        "    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "    model.to(device)\n",
        "    return model\n",
        "\n",
        "# Compute perplexity for evaluation\n",
        "def compute_perplexity(model, eval_dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Calculate number of tokens (all tokens are real, no padding)\n",
        "            num_tokens = attention_mask.sum().item()\n",
        "\n",
        "            # Sum up batch loss\n",
        "            total_loss += loss.item() * num_tokens\n",
        "            total_tokens += num_tokens\n",
        "\n",
        "    # Calculate perplexity\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "# Main training function\n",
        "def train():\n",
        "    tokenizer = get_tokenizer()\n",
        "    model = get_model()\n",
        "\n",
        "    # Load pre-tokenized datasets\n",
        "    train_dataset = RawTokenDataset(\"fixed_train.txt\")\n",
        "    val_dataset = RawTokenDataset(\"fixed_test.txt\")\n",
        "\n",
        "    logger.info(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "    # Create data collator\n",
        "    data_collator = DynamicCollator(MODEL_NAME)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        fp16=torch.cuda.is_available(),  # Use mixed precision training if available\n",
        "        dataloader_drop_last=True,      # Drop the last incomplete batch\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    logger.info(\"Starting training\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(\"./final_model\")\n",
        "    tokenizer.save_pretrained(\"./final_model\")\n",
        "\n",
        "    # Compute final perplexity\n",
        "    eval_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "    perplexity = compute_perplexity(model, eval_dataloader)\n",
        "    logger.info(f\"Final perplexity: {perplexity:.2f}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Generate some sample text\n",
        "def generate_sample(model, tokenizer, prompt=\"The music\", max_length=200):\n",
        "    model.eval()\n",
        "\n",
        "    # First tokenize the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate text using the maximum length possible\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    model, tokenizer = train()\n",
        "\n",
        "    # Generate some samples\n",
        "    print(\"\\nGenerated samples:\")\n",
        "    print(\"-\" * 40)\n",
        "    prompts = [\"The music\", \"I feel\", \"Love is\", \"Tonight we\"]\n",
        "\n",
        "    for prompt in prompts:\n",
        "        generated_text = generate_sample(model, tokenizer, prompt)\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Generated: {generated_text}\")\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "O5QxG-U2Np6q",
        "outputId": "c7f0d79f-efe8-4325-9aea-975e85ea0a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1668' max='1668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1668/1668 30:19, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.010700</td>\n",
              "      <td>2.474013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.876600</td>\n",
              "      <td>2.408950</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
            "Evaluating: 100%|██████████| 211/211 [00:34<00:00,  6.04it/s]\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated samples:\n",
            "----------------------------------------\n",
            "Prompt: The music\n",
            "Generated: The music is not loud but it's got a little bit of drama to me when you're standing here and i'm feeling like there ain't no way your eyes are gonna catch up with the words  pre my name isn 'bout what we gon' do in six months if she sayin that they can stay on this earth while everybody else think about us how could something as simple as love make sense for somebody so hard gotta see through all these walls tryna get down donít let go just wait til everything changes tell yourself nothing will ever change before one day people who'd never know better come along boy oh baby every single moment has been taken away from ya girl yeah hey now where am I supposedto start cause nobody knows more than them girls huh ehuh uhohhh hmmhmmm nope why dont be jealous please listen ima keep calling yo some bad ass bitch even though its true hes talking trash too much hate em ill find out someday soonhe'll have his\n",
            "----------------------------------------\n",
            "Prompt: I feel\n",
            "Generated: I feel like i'm on my last flight right now and you've probably never been this close before but the way your lips touch mine makes me wanna be closer yeah when she touches yours it's hard to keep from falling asleep so if there was a time in which we could all die for each other baby then why would anyone care where our bodies are at anyway  pre cause oh no don't let go just know that whatever is happening 'cause tonight ain' gonna bring us together again boy what do ya mean wait until tomorrow morning girl who'd rather have someone else than somebody nobody will love their family more damn well tell her leave them alone or risk losing everything forever holdin hands with some stupid shit still tryna find another man take his chances gettin killed by an assassin kill yourself first thing ooh he shoulda waited till next day how can they talk about things only yesterday got him feeling good fuckers hate themselves too long as nothing has changed because everybody knows here goes mr taylor j\n",
            "----------------------------------------\n",
            "Prompt: Love is\n",
            "Generated: Love is a big deal i've been through before in my life when you're around there's nothing like it with the girls here and at your party  pre that feeling of 'cause we love to dance all night so don't be scared if things start wrong take me home oh come back every time yeah tonight wonkydoo put our hands up kiss baby uh hold onto us until morning maybe tomorrow babe do something fun but boy did this girl feel good haha hey honey now can anyone please get off them legs let go where are they headed well look out for everybody else waitin' on ya damn man he ain on a list cause makin him know what his name means who knows right ahuh wanna see one more day eh um okay ok alright oooh uhhow ow yeeahh hello ho hoyohoookhey howdy everyone stop being rude just say hi goodbye thank god thats no excuse ladies dont act shady though im only tryna leave peace behind its\n",
            "----------------------------------------\n",
            "Prompt: Tonight we\n",
            "Generated: Tonight we came out of nowhere and it's all in our heads now what am i supposed to do with this shit that you said is cause if things don't change then maybe the whole world will but there ain' no way around your lies  hook oh yeah so go ahead look at me man how can I get closer than a little boy my love has got him looking good for real he knows when they're just friends too though she'll never know who turned her down 'cause sometimes im not even sure why its on or whatever thats gonna happen next girl babe tryna take care ooh ahah haha yeaokay again where did these lyrics come from huh let us start slow because before tonight everything ends once upon an island yes everybody gets mad right here inside everyone else dies well are still alive life as usual nothing lasts forever nobody understands anything anymore except those people talking their minds while others scream through silence wait until somebody comes along lets talk about hell more like heaven god help whoever needs some time save yourself\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating text file**"
      ],
      "metadata": {
        "id": "0hR1xD4EJpPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Read the CSV file\n",
        "print(\"Reading CSV file...\")\n",
        "df = pd.read_csv('combined_artists_tokenized.csv')\n",
        "\n",
        "# Check if Tokenized_Lyrics column exists\n",
        "if 'Tokenized_Lyrics' not in df.columns:\n",
        "    print(\"Column names in file:\", df.columns.tolist())\n",
        "    raise ValueError(\"The column 'Tokenized_Lyrics' was not found in the CSV file.\")\n",
        "\n",
        "print(f\"Found {len(df)} rows in the CSV file.\")\n",
        "\n",
        "# Create an output file to store all tokens\n",
        "output_file = 'all_tokens.txt'\n",
        "print(f\"Converting tokens to text file: {output_file}...\")\n",
        "\n",
        "# Process each row and write to the output file\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    # Counter for monitoring progress\n",
        "    total_tokens = 0\n",
        "\n",
        "    for i, row in enumerate(df.itertuples(), 1):\n",
        "        # Get the tokenized lyrics - handle different formats\n",
        "        try:\n",
        "            # If stored as string representation of list\n",
        "            if isinstance(row.Tokenized_Lyrics, str):\n",
        "                if row.Tokenized_Lyrics.startswith('[') and row.Tokenized_Lyrics.endswith(']'):\n",
        "                    tokens = ast.literal_eval(row.Tokenized_Lyrics)\n",
        "                else:\n",
        "                    # If it's just a string of space-separated tokens\n",
        "                    tokens = row.Tokenized_Lyrics.split()\n",
        "            # If already a list\n",
        "            elif isinstance(row.Tokenized_Lyrics, list):\n",
        "                tokens = row.Tokenized_Lyrics\n",
        "            else:\n",
        "                print(f\"Skipping row {i} - unexpected format: {type(row.Tokenized_Lyrics)}\")\n",
        "                continue\n",
        "\n",
        "            # Convert all tokens to strings and join with spaces\n",
        "            token_strings = [str(token) for token in tokens]\n",
        "            f.write(' '.join(token_strings) + '\\n')\n",
        "\n",
        "            # Update counter\n",
        "            total_tokens += len(tokens)\n",
        "\n",
        "            # Print progress every 1000 rows\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Processed {i} rows ({total_tokens} tokens so far)...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {i}: {e}\")\n",
        "\n",
        "# Print file stats\n",
        "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "print(f\"\\nDone! Created {output_file} with {total_tokens} total tokens.\")\n",
        "print(f\"File size: {file_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otvOOp_cKBz1",
        "outputId": "b7d5456e-5795-467c-f404-acf5bcdbdd6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV file...\n",
            "Found 6027 rows in the CSV file.\n",
            "Converting tokens to text file: all_tokens.txt...\n",
            "Processed 1000 rows (465794 tokens so far)...\n",
            "Processed 2000 rows (1137968 tokens so far)...\n",
            "Processed 3000 rows (1591275 tokens so far)...\n",
            "Processed 4000 rows (2003674 tokens so far)...\n",
            "Processed 5000 rows (2317327 tokens so far)...\n",
            "Processed 6000 rows (2702901 tokens so far)...\n",
            "\n",
            "Done! Created all_tokens.txt with 2712405 total tokens.\n",
            "File size: 11.63 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rEamFwnP1o78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r final_model.zip final_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQmxLWydP9-G",
        "outputId": "909558cf-dc0a-4e35-effc-3259898a4471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: final_model/ (stored 0%)\n",
            "  adding: final_model/model.safetensors (deflated 7%)\n",
            "  adding: final_model/tokenizer_config.json (deflated 56%)\n",
            "  adding: final_model/vocab.json (deflated 68%)\n",
            "  adding: final_model/generation_config.json (deflated 24%)\n",
            "  adding: final_model/merges.txt (deflated 53%)\n",
            "  adding: final_model/config.json (deflated 51%)\n",
            "  adding: final_model/special_tokens_map.json (deflated 74%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fix_token_file(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Reads the input file with space-separated tokens and reformats it properly.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to input file with space-separated tokens\n",
        "        output_file: Path to save the reformatted data\n",
        "    \"\"\"\n",
        "    logger.info(f\"Processing {input_file} -> {output_file}\")\n",
        "\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Clean and parse tokens\n",
        "    tokens = [token for token in content.split() if token.strip()]\n",
        "\n",
        "    try:\n",
        "        # Convert to integers to verify format\n",
        "        token_ids = [int(token) for token in tokens]\n",
        "        logger.info(f\"Successfully parsed {len(token_ids)} tokens from {input_file}\")\n",
        "\n",
        "        # Create context windows of appropriate size for GPT-2\n",
        "        max_seq_len = 1024  # GPT-2 context window\n",
        "        contexts = []\n",
        "\n",
        "        # Create sequences with stride\n",
        "        stride = 512  # 50% overlap\n",
        "        for i in range(0, len(token_ids) - max_seq_len + 1, stride):\n",
        "            end_idx = min(i + max_seq_len, len(token_ids))\n",
        "            contexts.append(token_ids[i:end_idx])\n",
        "\n",
        "        # If there's a remainder and it's not too small, add it as the last context\n",
        "        if len(token_ids) % stride > 100:  # Only add if it's a substantial chunk\n",
        "            contexts.append(token_ids[-(len(token_ids) % stride):])\n",
        "\n",
        "        logger.info(f\"Created {len(contexts)} sequences of maximum length {max_seq_len}\")\n",
        "\n",
        "        # Write to output file - one sequence per line\n",
        "        with open(output_file, 'w') as f:\n",
        "            for context in contexts:\n",
        "                f.write(' '.join(map(str, context)) + '\\n')\n",
        "\n",
        "        logger.info(f\"Saved {len(contexts)} sequences to {output_file}\")\n",
        "        return True\n",
        "\n",
        "    except ValueError as e:\n",
        "        logger.error(f\"Error converting tokens to integers: {e}\")\n",
        "        logger.error(f\"First 10 tokens: {tokens[:10]}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fix both train and test files\n",
        "    train_success = fix_token_file(\"train.txt\", \"fixed_train.txt\")\n",
        "    test_success = fix_token_file(\"test.txt\", \"fixed_test.txt\")\n",
        "\n",
        "    if train_success and test_success:\n",
        "        logger.info(\"Successfully reformatted both train.txt and test.txt\")\n",
        "        logger.info(\"Use fixed_train.txt and fixed_test.txt with your model\")\n",
        "    else:\n",
        "        logger.error(\"Failed to process one or both files\")"
      ],
      "metadata": {
        "id": "yAtk7M7KOTgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zk5PL8-xkC62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERATING MUSIC LYRICS**"
      ],
      "metadata": {
        "id": "Wn89vCWY1taq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_file.zip' with your zip file name\n",
        "!unzip final_model.zip -d ./\n",
        "\n",
        "# List the extracted files\n",
        "!ls -la ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4KNm2kD13rr",
        "outputId": "16466ec7-3adb-41d5-9208-f768beec0ba5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  final_model.zip\n",
            "   creating: ./final_model/\n",
            "  inflating: ./final_model/model.safetensors  \n",
            "  inflating: ./final_model/tokenizer_config.json  \n",
            "  inflating: ./final_model/vocab.json  \n",
            "  inflating: ./final_model/generation_config.json  \n",
            "  inflating: ./final_model/merges.txt  \n",
            "  inflating: ./final_model/config.json  \n",
            "  inflating: ./final_model/special_tokens_map.json  \n",
            "total 452060\n",
            "drwxr-xr-x 1 root root      4096 Feb 27 19:20 .\n",
            "drwxr-xr-x 1 root root      4096 Feb 27 19:15 ..\n",
            "drwxr-xr-x 4 root root      4096 Feb 26 18:27 .config\n",
            "drwxr-xr-x 2 root root      4096 Feb 27 08:26 final_model\n",
            "-rw-r--r-- 1 root root 462881440 Feb 27 19:20 final_model.zip\n",
            "drwxr-xr-x 1 root root      4096 Feb 26 18:27 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "akBg4CvH2Z1b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./final_model\"  # Update this path to where your model is\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_fHR7uP43VDc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghr-45lp6P-h",
        "outputId": "9404d51f-d7ef-4bcc-e480-0ecb30962bd8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, max_length=100):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.8,  # Controls randomness (higher = more random)\n",
        "        top_k=50,         # Controls diversity\n",
        "        top_p=0.95,       # Nucleus sampling\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True    # Use sampling instead of greedy decoding\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "print(\"Enter song lyrics and I'll continue them. Type 'quit' to exit.\")\n",
        "while True:\n",
        "    prompt = input(\"\\nEnter your music lines: \")\n",
        "\n",
        "    if prompt.lower() == 'quit':\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    generated_text = generate_text(prompt)\n",
        "\n",
        "\n",
        "\n",
        "    # Print character by character with delay for typewriter effect\n",
        "    for char in generated_text:\n",
        "        print(char, end='', flush=True)  # flush=True ensures it prints immediately\n",
        "        time.sleep(0.03)  # 30 milliseconds delay between characters\n",
        "\n",
        "    print()  # Add a newline at the end\n",
        "    # Print just the newly generated part (excluding the prompt)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "4Tk1AY-63qfM",
        "outputId": "66a5b39a-9231-48df-ef46-274ab4753459"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter song lyrics and I'll continue them. Type 'quit' to exit.\n",
            "\n",
            "Enter your music linesq: i got her flowers\n",
            "i got her flowers we used to make it and i had my heart on the edge of town so come with me uh  pre cause you can be crazy that's what they say baby don't even try but if somebody ever decides there are no other options then oh hey yeah now get off your feet just put 'em down okay all good for ya look at these little bitches how'd yo love themselves girl this could happen right here in front ooh hello goooooooohooyuhh\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7c90d8d960e9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter song lyrics and I'll continue them. Type 'quit' to exit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter your music linesq: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDzMT6rS583p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}